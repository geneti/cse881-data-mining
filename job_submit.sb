#!/bin/bash --login
########## SBATCH Lines for Resource Request ##########

# limit of wall clock time - how long the job will run (same as -t)
#SBATCH --time=00:30:00

# number of different nodes - could be an exact number or a range of nodes 
#SBATCH --nodes=1

#SBATCH --gres=gpu:k80:1

# memory required per allocated  Node  - amount of memory (in bytes)
#SBATCH --mem=100G                  

#Send email notification to your MSU email when the job begins, ends, or is aborted by the scheduler.
#SBATCH --mail-user=chenzho7@msu.edu   
#SBATCH --mail-type=FAIL,BEGIN,END

# you can give your job a name for easier identification (same as -J)
#SBATCH --job-name CSE881_proj
#SBATCH --output /mnt/gs18/scratch/users/chenzho7/cse881/slurm_output/%j.out
 
########## Command Lines to Run ##########


cd $SLURM_SUBMIT_DIR


model="blstm"

# model
mode="--mode train"
data_path="--data_path ./src_zzc/data_ensemble/Training.txt"
res_root="--res_root /mnt/gs18/scratch/users/chenzho7/cse881/res_ensemble"
fold="--fold 2"
seed="--seed $SLURM_JOBID"

# nn
epoch="--epoch 20"
batch_size="--batch_size 50"
lr="--lr 0.01"
desired_len_percent="--desired_len_percent 1.0"
optimizer="--optimizer SGD"
activate_func="--activate_func Tanh"

# dan
dan_hidden_sz="--dan_hidden_sz 100"
dan_num_hidden="--dan_num_hidden 3"

# embedding
emb_drop_r="--emb_drop_r 0.0"
emb_size="--emb_size 200"


# LSTM
lstm_drop_r="--lstm_drop_r 0.2"
lstm_n_layer="--lstm_n_layer 1"
lstm_hidden_sz="--lstm_hidden_sz 100"

# cnn
cnn_n_kernel="--cnn_n_kernel 50"
cnn_kernel_sz="--cnn_kernel_sz 3"
cnn_pool_kernel_sz="--cnn_pool_kernel_sz 2"

command="srun --gres=gpu:1 ./venv/bin/python ./src_zzc/main.py $model --is_disk_limited --is_redirected $mode $data_path $res_root $fold $seed $epoch $batch_size $lr $desired_len_percent $optimizer $activate_func $dan_hidden_sz $dan_num_hidden $emb_drop_r $emb_size $lstm_drop_r $lstm_n_layer $lstm_hidden_sz $cnn_n_kernel $cnn_kernel_sz $cnn_pool_kernel_sz"

echo "Running "
echo $command
echo ""
echo ""

eval $command

echo ""
echo "Done"